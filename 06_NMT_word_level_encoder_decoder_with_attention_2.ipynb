{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Machine Translation with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from unicodedata import normalize\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import AdditiveAttention\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import AdditiveAttention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "import typing\n",
    "from typing import Any, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"dataset/spa.txt\"\n",
    "MAX_LINES = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "units = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "def load_data(path):\n",
    "    text = path.read_text(encoding='utf-8')\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    pairs = [line.split('\\t')[:2] for line in lines]\n",
    "\n",
    "    inp = [inp for targ, inp in pairs]\n",
    "    targ = [targ for targ, inp in pairs]\n",
    "\n",
    "    return targ, inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pathlib.Path('')\n",
    "path_to_file = p/'dataset/spa.txt'\n",
    "targ, inp = load_data(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = tf.constant('¿Todavía está en casa?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(inp)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'Toma todo el tiempo que necesites.'\n",
      " b'Puedes tomar un paraguas si necesitas uno.' b'Yo hablo conmigo mismo.'\n",
      " b'Estoy feliz de que Tom est\\xc3\\xa9 feliz.'\n",
      " b'\\xc2\\xbfPuedo pagar el libro con un cheque?'], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b'Take all the time you need.'\n",
      " b'You can borrow an umbrella if you need one.' b'I talk to myself.'\n",
      " b\"I'm happy that Tom is happy.\" b'Can I pay for the book by check?'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example_input_batch, example_target_batch in dataset.take(1):\n",
    "    print(example_input_batch[:5])\n",
    "    print()\n",
    "    print(example_target_batch[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    # Split accecented characters.\n",
    "    text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "    text = tf.strings.lower(text)\n",
    "    # Keep space, a to z, and select punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "    # Add spaces around punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "    # Strip whitespace.\n",
    "    text = tf.strings.strip(text)\n",
    "\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 5000\n",
    "\n",
    "input_text_processor = preprocessing.TextVectorization(standardize=tf_lower_and_split_punct, \n",
    "                                                       max_tokens=max_vocab_size)\n",
    "input_text_processor.adapt(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text_processor = preprocessing.TextVectorization(standardize=tf_lower_and_split_punct, \n",
    "                                                       max_tokens=max_vocab_size)\n",
    "output_text_processor.adapt(targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 10), dtype=int64, numpy=\n",
       "array([[   2,  705,   57,    7,   60,    5, 2960,    4,    3,    0],\n",
       "       [   2,  101,  314,   16,  808,   44,  420,  179,    4,    3],\n",
       "       [   2,   39,  504,  223,  161,    4,    3,    0,    0,    0]],\n",
       "      dtype=int64)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tokens = input_text_processor(example_input_batch)\n",
    "example_tokens[:3, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[START] toma todo el tiempo que necesites . [END]           '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
    "tokens = input_vocab[example_tokens[0].numpy()]\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Mask')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcRUlEQVR4nO3de5RdZX3/8fd3LslMLlwmN0NIiJCIIkggISCpClKUAi2xa0GxVvPT0PSCVl3aSksL1NXlol1W0aWr/lJAoiIloqykXf2pkBrRnwIhMYRLgACGEDLNQO7Bgcyc+faPs0MPYfazz5zrPs98XmvNmjn7e569n5w8853nfM+z9zZ3R0RE4tLW7A6IiEjtKbmLiERIyV1EJEJK7iIiEVJyFxGJkJK7iEiElNzryMzOM7Ptze6HSKsxs7VmdlWz+9HKlNzLZGYHS76GzKy/5PGHmty3134Rkj8oQyV9225mK83srGb2UeJjZlvN7JCZTT5i+0YzczOb3aSuCUruZXP3CYe/gG3A75Zsu73Z/TvCjqSfE4FzgCeAn5nZBc3tlkTo18AHDz8ws9OA7uZ1Rw5Tcq+SmY01s5vMbEfydZOZjU157l+Y2eNmdnzS7otmts3MdprZN8ysO3neecmM+zNm1mdmvWb20ZH2zYu2u/t1wM3APyb7NzP7crLvfWa2ycxOreZ1kFHr28BHSh4vAb51+IGZXWJmvzKz/Wb2vJndUBLrMrPvmNkuM9trZuvMbNqRBzCz6ckY/Ww9/yGxUXKv3rUUZ8fzgNOBhcDfHvkkM/s74P8A73H37RQT7VuSdnOAGcB1JU3eBBydbF8KfN3Mjq2inz8AzjSz8cD7gHcnxz8G+ANgVxX7ltHrfuAoM3ubmbVTHEvfKYm/TDH5HwNcAvyZmS1OYksojvGZwCTgT4H+0p0npZ2fAl9z9y/W7V8RISX36n0I+Ly797n7i8DfAx8uiZuZfQl4P3C+u79oZgb8MfBpd9/t7geALwBXlrQbSPY74O7/CRwETq6inzsAo/hLNkCxZPNWwNx9s7v3VrFvGd0Oz94vpFgCfOFwwN3Xuvsj7j7k7puAO4D3JOEBikl9jrsX3H29u+8v2e8pwFrgendf3oB/R1Q6mt2BCBwHPFfy+Llk22HHAMuAP3D3fcm2KcA4YH0xzwPFxNte0m6Xuw+WPP4NMKGKfs4AHNjr7v9lZl8Dvg7MMrO7gc8e8YslUq5vA/cBb6akJANgZmcDNwKnAmOAscD3StrNBP7NzI6hOOO/1t0HkviHgKeBu+rc/yhp5l69HcAJJY9nJdsO2wNcCnzTzBYl216i+Pbz7e5+TPJ1dPIhaL18ANjg7i8DuPtX3X0+8HaK5Zm/rOOxJWLu/hzFD1Yvplj+K/VdYDUw092PBr5BcSJD8q707939FOBcir8npfX7Gyj+rnw3KfnICCi5V+8O4G/NbEqyJOw6Xl9zxN3XUpyF3G1mZ7v7EPCvwJfNbCqAmc0ws/fXsmPJB6czzOx64Crgb5LtZ5nZ2WbWSbEm+gpQqOWxZdRZCrz38OShxERgt7u/YmYLgT88HDCz883stCRx76dYpikdhwPA5cB44Ntmpnw1AnqxqvcPwEPAJuARYEOy7XXc/R7go8BqM5sPfI7iW877zWw/cC/V1dRLHWdmBynW6dcBpwHnufuPk/hRFP+47KFYRtoF6MMqqZi7P+PuDw0T+nPg82Z2gOLEZ2VJ7E0USy77gc0UPzg9cmJ0CPh9YCpwqxJ8+Uw36xARiY/+CoqIREjJXUQkQkruIiIRUnIXEYlQQ09iGmNjvYvxw8ZKTuYZVuiDX2vL+huV3taH9IFyLA6w5yV3n9KMY0/uaffZMzubceimeGrTuGZ3YVSpZGw3NLl3MZ6zUy5M2Da2K9h26NVXUmNt3RkDLfCHYai/PzUmreVev+u57GfVx+yZnTz4o1nNOnzDvf+405vdhVGlkrGtsoyISISU3EVEIpSbC4f54ED2k1IM9aeXbABO35Aee/iMig8rEi2VXVqfZu4iIhFSchcRiZCSu4hIhHJTc7f28OWavZB+Rdq2rmFvWfqah89IX+7YMfuE1BhA4fntFfeLjAvYdcyaEYwPPvd8MC5SLz/a8XCzuzAsfRZQPs3cRUQipOQuIhKhssoyyf0Nb6Z4H0QHPgY8CdwJzAa2Ale4+56Ke1LFNfhffdfbg/HOe9LXQg5ureNJjT4UDKvs0nwNGdsRUnkk/8rNqF8BfujubwVOp3jXlGuANe4+F1iTPBZpNRrbEqXM5G5mRwHvBm6B4m2v3H0vcBmwInnaCmBxfbooUh8a2xKzcsoyJwIvAt80s9OB9cAngWnu3gvg7r2Hb/R8JDNbBiwD6CL9Al+hC4MBtI0ZkxrrevDpYNtCRnmkGm3d3akxHxgMN87oV/uJs1Njg1ueCe9bylGzsT1rRm4WnjWEVtPkXzllmQ7gTOBf3P0M4GVG8DbV3Ze7+wJ3X9BJeMmiSIPVbGxPmRReyivSaOUk9+3Adnd/IHl8F8VfiJ1mNh0g+d5Xny6K1I3GtkQrM7m7+38Dz5vZycmmC4DHgdXAkmTbEmBVXXooUica2xKzcguFnwBuN7MxwLPARyn+YVhpZkuBbcDlVXVk1vHB+OC29DNFs/5CtU+YkBorHDyY0Tqsnjf7UF29Ieo+tkcj1b6br6zk7u4bgQXDhIa/rZJIi9DYlljpDFURkQjlZv1WqOwCsO2Gc1NjX/vw8mDbfzrp1NTYllvOCradu3RdML77qvR+9dz8i2DbrLNyJ9w3OTV28F36jE/yq1lLJVUO+l+auYuIREjJXUQkQkruIiIRyk3NPcvnrvx+aixUUweCte2smnqWzLp6SMblB1RXl2ZR7br1aeYuIhIhJXcRkQjlpizTMSV92R/AHW9Nj736uwuDbYfaLTU27t8fCrbd8s3w29M5H0m/EYhIq6p2KaPKOs2nmbuISISU3EVEIpSbsszgS7uD8bb56Stiutc8Gt55IX1VylChEGyqsouMRiqrtD7N3EVEIqTkLiISISV3EZEI5abm3vPzY4Px3Ysy6uoB+z7yztTY0d/6ZcX7FRHJK83cRUQipOQuIhKh3JRldi/aVXHbrV9Iv2EGwOxr70+NtXV3B9vW8x6pInmVdYaqlkrmn2buIiIRUnIXEYmQkruISIRyU3PPYh2dqbETbwhf2dHb21NjqqmLvJFq6q1PM3cRkQgpuYuIRKissoyZbQUOAAVg0N0XmFkPcCcwG9gKXOHueyrtSPuECcF44eDB1Jhn7Ltt3Lj0486ZHWw7+MSWjL2n231VeIlmVfdflZpoxNhuRdXerKMaKgnVxkhm7ue7+zx3X5A8vgZY4+5zgTXJY5FWpLEt0ammLHMZsCL5eQWwuOreiOSDxra0vHJXyzjwYzNz4P+6+3Jgmrv3Arh7r5lNHa6hmS0DlgF0kV4eCZVdAPZ8LP3iXz3fCq+WsRNnpsaGnvp1sG01VHZpCTUZ27NmtMzCs7KoNNL6yh2Ri9x9RzLI7zGzJ8o9QPLLshzgKOvJKo+LNFpNxvaC07s0tiVXyirLuPuO5HsfcDewENhpZtMBku999eqkSL1obEusMpO7mY03s4mHfwbeBzwKrAaWJE9bAqyqVydF6kFjW2JWTllmGnC3mR1+/nfd/Ydmtg5YaWZLgW3A5dV0pG1sVzA+5Wfpk6d9i+cH246/K/2qkFjzlvpb4MxZgG3Xnp0am/l51fNroCFjuxU1cylkNfRZwf/KTO7u/izwhlfM3XcBF9SjUyKNoLEtMdMZqiIiEcrN+q2hV18Jx59OX7I4fsszwbbtRx+dGvM56cskAYbWV37v1ixeKATjKr1IXqn8kX+auYuIREjJXUQkQkruIiIRyk3NvZ5CdXV/+Mlg26zlill1c5EY6Qba+aeZu4hIhJTcRUQi1DplGR9KDYXurwrVLWcsnB8++7X9J+sr3reISL1o5i4iEiEldxGRCOWnLJNxAa/QqpUXrzor2Hbqrek38xgaGAy2VdlFRFqRZu4iIhFSchcRiZCSu4hIhPJTcw8sdQTwwImgoZo6hOvqHce9Kdh28IUdwbjIaKQzUPNPM3cRkQgpuYuIRCg3ZZmse6j2X/iO1Nj4Dc8H2w7t6E0PdoePm2XgnhNSY50XPlfVvkXyqpp7rKqk0xiauYuIREjJXUQkQkruIiIRyk3NPesG2WP/48HU2GDGpQueuf2M1NhJH/pVuGMZVFcXeSPV1ZtPM3cRkQgpuYuIRKjssoyZtQMPAS+4+6Vm1gPcCcwGtgJXuPueSjuSdcMNHxxIjXVM7gm2fcvVv04/7pTJwbaFPfsq7leWjmlTg/HBnX0V71vKU+9xPVpVs1QyROWe8o1k5v5JYHPJ42uANe4+F1iTPBZpNRrXEqWykruZHQ9cAtxcsvkyYEXy8wpgcU17JlJnGtcSs3LLMjcBfwVMLNk2zd17Ady918yGrTGY2TJgGUAX41IP4IXAlcGAtu7u1Njgiy8F2+7/o3emxo76zi+DbetJZZemu4kKxzW8fmzPmpGbhWc1ofJH68ucuZvZpUCfu1d0SyJ3X+7uC9x9QSdjK9mFSM1VO67h9WN7yqT0O4WJNEM5041FwO+Z2cVAF3CUmX0H2Glm05PZzXRA01BpJRrXErXMmbu7/7W7H+/us4Ergf9y9z8CVgNLkqctAVbVrZciNaZxLbGrplB4I7DSzJYC24DLa9Ol4dmYMenB/v5g21BdvePkOcG2g08+He5X4Mbdf7clfBORGz62NBhvW7shGJe6aOi4zqt6LWWslj4LKN+Ikru7rwXWJj/vAi6ofZdEGkvjWmKkM1RFRCLUMuu3bEz6GaxZN/oInUWaWXbJOHM2dO/X23elL8EElV1EhqPSS21o5i4iEiEldxGRCCm5i4hEKD8190DtGmDwpd0VtyVwM4+2+acGmw6tfzS874BnzgpfUkGkVakunn+auYuIREjJXUQkQvkpy2QInQm6Z9VJwbbHXPpMaiyr7HLhoweD8Z+854TU2OCuXcG2Iq2qmjNYVdJpDM3cRUQipOQuIhKh3JRlht5zZjDe9tP0szl7PvBceN+B1TQdJ8wMtr3n1OeDcVDpRUYflVbyTzN3EZEIKbmLiERIyV1EJEK5qbmHauoAz/5T+hUWT/zcA+F9B64a+bF7fxps+97uncH4lceHr/woEqOspZCqyTefZu4iIhFSchcRiVBuyjKhM1AB5ly/MTU2dPZpwbZD96e/hbzlzPDbx+UHXw7GD/3O/NTYmB+uD7bNuuBZx6RJqbHMs1/PnZe+3yfDyzuHDhwIxw8dCh874NkvBsprn02/1620lrzeg7VVtU8feRvN3EVEIqTkLiISISV3EZEItUzNvW3a1NRYqKaepZBRX84y5v+tq6p9SFVXlfzFxvT9Vr7Xqqmu3jhajhiTLSNuoZm7iEiElNxFRCKUWZYxsy7gPmBs8vy73P16M+sB7gRmA1uBK9x9T6UdGRoIFwv6LpqRGpv8jfBVIasx8L4FwXjXC/tTY4XHnqp1d6SGGjW2myWvyxFVLmqMcmburwLvdffTgXnARWZ2DnANsMbd5wJrkscirURjW6KVmdy96PC95jqTLwcuA1Yk21cAi+vRQZF60diWmJW1WsbM2oH1wBzg6+7+gJlNc/deAHfvNbNhl7OY2TJgGUAX49IPknG2Zs/j/eV0dVihC4dt+cd5wbYnffrB8M6PmlhBjyQvajW2Z83IzcIzEaDMD1TdveDu84DjgYVmdmq5B3D35e6+wN0XdDK2wm6K1EetxvaUSeGlvCKNNqLVMu6+F1gLXATsNLPpAMn3vlp3TqRRNLYlNpnJ3cymmNkxyc/dwG8DTwCrgSXJ05YAq+rUR5G60NiWmJVTKJwOrEhqk23ASnf/DzP7JbDSzJYC24DLq+mILQxf2bH9/29Kjb30J+cG20755kOpsZM+dX+4YxkK+/ZV1V6aqiFjuxVpuWLry0zu7r4JOGOY7buAC+rRKZFG0NiWmOkMVRGRCOVm/ZaveywY/8Dj6Z9p/fuF4TNUB0M3l7Dw37f5vyoE4+vnWTAect4j4eWda0/rTg9m9Lvn58emxnYvquKCZDIq5PXs1nqKrRSlmbuISISU3EVEIqTkLiISodzU3NvHBy5NAKw6/bjUmBf+u9bdec36M7LOPAxfNiEkWFOHYF29Y/q0YNPdi3or6ZJI1GKrq4do5i4iEiEldxGRCOWmLFM4eDAYH3j/Wamxzh9l3Mc0UN7o+8Q5waZTv/qLYLxtXHo5aaj/lWDb/X94djB+1O3p9xsd3KGyi4w+o6msUi3N3EVEIqTkLiISodyUZSb8bNj7Ibzm4LsySi8hgRuBZJVdsgz95jcVtw2VXUTKoTKFpNHMXUQkQkruIiIRUnIXEYlQbmruL5+/JxjvmDolNTbY92LlB864uuJ1z6wPxj9/4hsuBy7SMHm9eqM+C2g+zdxFRCKk5C4iEqHclGUGzg+/jds1a0xqrOfWjJtPBJZCBmOo7CKjk8oqrU8zdxGRCCm5i4hESMldRCRCuam5d9y7IRjveec7UmPWFr5J9cHfT7/y4/jv3R/umIhIC9LMXUQkQkruIiIRyizLmNlM4FvAmyjeMHS5u3/FzHqAO4HZwFbgCncPn2YakrEk0R54JL1poRBs++5r06++uP574ZJOx0lvDsbZuz81NNTfH27rHgxntpeqNGxstyCd+dr6ypm5DwKfcfe3AecAV5vZKcA1wBp3nwusSR6LtBKNbYlWZnJ3915335D8fADYDMwALgNWJE9bASyuUx9F6kJjW2I2otUyZjYbOAN4AJjm7r1Q/CUxs2HvtmFmy4BlAF2k32806wJebRMmpDftGhtsu35eX2psyzcWBtvO/dMHg3GJQ7Vje9aM3Cw8awiVR/Kv7A9UzWwC8H3gU+6eXmg+grsvd/cF7r6gk3ASFmmGWoztKZPa69dBkQqUldzNrJPi4L/d3X+QbN5pZtOT+HQgfXosklMa2xKrzORuZgbcAmx29y+VhFYDS5KflwCrat89kfrR2JaYlVMoXAR8GHjEzDYm2/4GuBFYaWZLgW3A5dV0xNrDb2sPnTknNdb+k/ANNUJUUx/VGjK2m0V18dEtM7m7+8+BtMXgF9S2OyKNo7EtMdMZqiIiEcrN+q2ss0zHPPxsamwoo6Rj7zg5ve2vHg+3zdh3Vr9FmqWZZ5mqJNR8mrmLiERIyV1EJEJK7iIiEcpNzb2tM9yVwp59qbFPP/1YsO2X54SvvhiimrrIG6mmnn+auYuIREjJXUQkQrkpy9AR7ooFyiM3veXU8K5nTk+NDW7bHu6XyCikskvr08xdRCRCSu4iIhHKT1lmcDAYDq1aaevuDrfdvbeSHomItCzN3EVEIqTkLiISISV3EZEI5afmXoWh/v5md0EkKllXlNRSyfzTzF1EJEJK7iIiEcpNWWbo0KGK2+74y3OD8RlfeiA1lnlhMAv//euYMik1Ntj3YnjfIi2qmhuBqKTTGJq5i4hESMldRCRCSu4iIhHKTc09S/tpb02NhWrqAJ94Mv0m2F+dk37zbAB8KBhWXV1ipLp469PMXUQkQkruIiIRyizLmNmtwKVAn7ufmmzrAe4EZgNbgSvcfU/9ugnWtzs11n/J/GDbr84JLHfMWOpobRaM6x6rrSsvYzuPqlnqCCrr5EE5M/fbgIuO2HYNsMbd5wJrkscireY2NLYlUpnJ3d3vA46cNl8GrEh+XgEsrm23ROpPY1tiVulqmWnu3gvg7r1mNjXtiWa2DFgG0MW41B22jRkTPODm696cGpt7dXi1TPvEiamxrLLK4ML0VToAbWs3BOPScioa27NmtMzCMxkl6v6Bqrsvd/cF7r6gk7H1PpxIw5SO7SmT2pvdHZHXqTS57zSz6QDJ977adUmkqTS2JQqVJvfVwJLk5yXAqtp0R6TpNLYlCuUshbwDOA+YbGbbgeuBG4GVZrYU2AZcXm1H2o6bHozP/fi6ivddOPhyejDjDNTOlwJtAS2EbF2NGtsizZCZ3N39gymhC2rcF5GG0tiWmOkMVRGRCOVm/dbg1ueC8faeY1Njhd0ZJxBmlF5CCo8+WXFbkWbSWaKjm2buIiIRUnIXEYmQkruISIRyU3PPYh0t01WRXMi6sqNq8nHTzF1EJEJK7iIiEcpNreOV31sYjHetfjA1prefIiKvp5m7iEiElNxFRCKUm7JMqOwCYB2dqbGLL8y6ttNTqZG27u5gy6FXXg3vOnD26ynrwy/v4/MHw/sWqaNq75NaLyqj1oZm7iIiEVJyFxGJkJK7iEiEclNzt/bwPSjbJoxPjRUeS6+pZxnq7w/GM/s18ZjU2OPz91bQI5G4qabeGJq5i4hESMldRCRCuSnLZLFpk1Nj7W0WbjyQvuSwcOBAsKkXwndJLezdGz62SAtS6aT1aeYuIhIhJXcRkQgpuYuIRCg3NffQ5QUA2H8wNZR1g2z/rXmpsbZfPhJs2z7z+PC+9+xNjRX27Qu2Fcmrel6aQPX8xtDMXUQkQkruIiIRqqosY2YXAV8B2oGb3f3GSvflp80Nxgsbn6h017Td/1j6cTOWOu78WlcwPulSlV5iVMuxPRqp9NJ8Fc/czawd+DrwO8ApwAfN7JRadUykWTS2JQbVlGUWAk+7+7Pufgj4N+Cy2nRLpKk0tqXlVVOWmQE8X/J4O3D2kU8ys2XAsuThq/f6XY8Ou7d1d1XRlQwDmc+YDLw0bOSSGvdlZNL71Vx57dfJNdpPRWO7ffqW4cd2czXp/2pL1hPyOoby2q8Rj+1qkvtw5/z7Gza4LweWA5jZQ+6+oIpj1oX6NTJ57letdjXMNo3tGlK/RqaSsV1NWWY7MLPk8fHAjir2J5IXGtvS8qpJ7uuAuWb2ZjMbA1wJrK5Nt0SaSmNbWl7FZRl3HzSzjwM/orhc7FZ3T19zWLS80uPVmfo1MlH3S2O7IdSvkRlxv8z9DaVEERFpcTpDVUQkQkruIiIRakhyN7OLzOxJM3vazK5pxDHLZWZbzewRM9tYw6V0lfTjVjPrM7NHS7b1mNk9ZrYl+X5sTvp1g5m9kLxmG83s4ib0a6aZ/cTMNpvZY2b2yWR7Q1+zvI5tjeuK+hXVuK57cm+RU7nPd/d5TV7fehtw0RHbrgHWuPtcYE3yuNFu4439Avhy8prNc/f/bHCfAAaBz7j724BzgKuTcdWw16wFxrbGdbrbiHxcN2LmrlO5y+Du9wG7j9h8GbAi+XkFsLiRfYLUfjWdu/e6+4bk5wPAZopnljbyNdPYzqBxPTK1HNeNSO7Dnco9owHHLZcDPzaz9cnp5Hkyzd17ofifDkxtcn9KfdzMNiVvbxv+trqUmc0GzgAeoLGvWZ7HtsZ1ZaIZ141I7mWdyt1Ei9z9TIpvra82s3c3u0Mt4F+Ak4B5QC/wz83qiJlNAL4PfMrd9zf68MNsy8vY1rgeuajGdSOSe65P5Xb3Hcn3PuBuim+182KnmU0HSL73Nbk/ALj7TncvuPsQ8K806TUzs06KvwC3u/sPks2NfM1yO7Y1rkcutnHdiOSe21O5zWy8mU08/DPwPiBPV/ZbDSxJfl4CrGpiX15zeJAlPkATXjMzM+AWYLO7f6kk1MjXLJdjW+O6MtGNa3ev+xdwMfAU8AxwbSOOWWa/TgQeTr4ea2bfgDsovhUcoDgjXApMovjJ+Jbke09O+vVt4BFgUzLopjehX79FsQSyCdiYfF3c6Ncsj2Nb47rifkU1rnX5ARGRCOkMVRGRCCm5i4hESMldRCRCSu4iIhFSchcRiZCSu4hIhJTcRUQi9D9FudMveoaIwQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(example_tokens)\n",
    "plt.title('Token IDs')\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(example_tokens != 0)\n",
    "plt.title('Mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(keras.layers.Layer):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.embedding = Embedding(self.input_vocab_size, embedding_dim)\n",
    "        self.gru = GRU(self.enc_units,\n",
    "                      return_sequences=True,\n",
    "                      return_state=True,\n",
    "                      recurrent_initializer='glorot_uniform')\n",
    "            \n",
    "    def call(self, tokens, state=None):\n",
    "        vectors = self.embedding(tokens)\n",
    "        output, state = self.gru(vectors, initial_state=state)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch, shape (batch): (64,)\n",
      "Input batch tokens, shape (batch, s): (64, 20)\n",
      "Encoder output, shape (batch, s, units): (64, 20, 256)\n",
      "Encoder state, shape (batch, units): (64, 256)\n"
     ]
    }
   ],
   "source": [
    "# Convert the input text to tokens.\n",
    "example_tokens = input_text_processor(example_input_batch)\n",
    "\n",
    "# Encode the input sequence.\n",
    "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "                  embedding_dim, \n",
    "                  units)\n",
    "\n",
    "example_enc_output, example_enc_state = encoder(example_tokens)\n",
    "\n",
    "print(f'Input batch, shape (batch): {example_input_batch.shape}')\n",
    "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
    "print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
    "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.W1 = Dense(units, use_bias=False)\n",
    "        self.W2 = Dense(units, use_bias=False)\n",
    "        self.attention = AdditiveAttention()\n",
    "        \n",
    "    def call(self, query, value, mask):\n",
    "        w1_query = self.W1(query)\n",
    "        w2_key = self.W2(value)\n",
    "        \n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "    \n",
    "        context_vector, attention_weights = self.attention(inputs=[w1_query, w2_key],\n",
    "                                                           mask=[query_mask, value_mask],\n",
    "                                                           return_attention_scores =True)\n",
    "        return context_vector, attention_weights\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch_size, query_seq_length, units): (64, 2, 256)\n",
      "Attention weights shape: (batch_size, query_seq_length, value_seq_length): (64, 2, 20)\n",
      "(64, 2, 20)\n",
      "(64, 20, 256)\n",
      "(64, 2, 256)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(units)\n",
    "\n",
    "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 20])\n",
    "context_vector, attention_weights = attention_layer(query=example_attention_query,\n",
    "                                                    value=example_enc_output,\n",
    "                                                    mask=(example_tokens != 0))\n",
    "\n",
    "print(f'Attention result shape: (batch_size, query_seq_length, units): {context_vector.shape}')\n",
    "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')\n",
    "\n",
    "print(example_attention_query.shape)\n",
    "print(example_enc_output.shape)\n",
    "print(context_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Mask')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(attention_weights[:, 0, :])\n",
    "plt.title('Attention weights')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(example_tokens != 0)\n",
    "plt.title('Mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderInput(typing.NamedTuple):\n",
    "    new_tokens: Any\n",
    "    enc_output: Any\n",
    "    mask: Any\n",
    "\n",
    "class DecoderOutput(typing.NamedTuple):\n",
    "    logits: Any\n",
    "    attention_weights: Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(keras.layers.Layer):\n",
    "    def __init__(self, target_vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = Embedding(self.target_vocab_size, \n",
    "                                   embedding_dim)\n",
    "        self.gru = GRU(self.dec_units,\n",
    "                      return_sequences=True,\n",
    "                      return_state=True,\n",
    "                      recurrent_initializer='glorot_uniform')\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "        #  converting context vector to attention vector\n",
    "        self.Wc = Dense(dec_units,\n",
    "                       activation=tf.math.tanh,\n",
    "                       use_bias=False)\n",
    "        # producing logits\n",
    "        self.fc = Dense(self.target_vocab_size)\n",
    "        \n",
    "    def call(self, \n",
    "             inputs:DecoderInput, \n",
    "             state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
    "        vectors = self.embedding(inputs.new_tokens)\n",
    "        rnn_output, state = self.gru(vectors, initial_state=state)\n",
    "        context_vector, attention_weights = self.attention(query=rnn_output,\n",
    "                                                          value=inputs.enc_output,\n",
    "                                                          mask=inputs.mask)\n",
    "        context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
    "        attention_vector = self.Wc(context_and_rnn_output)\n",
    "        logits = self.fc(attention_vector)\n",
    "        \n",
    "        return DecoderOutput(logits, attention_weights), state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                  embedding_dim, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: (batch_size, t, output_vocab_size) (64, 1, 5000)\n",
      "state shape: (batch_size, dec_units) (64, 256)\n"
     ]
    }
   ],
   "source": [
    "example_output_tokens = output_text_processor(example_target_batch)\n",
    "start_index = output_text_processor._index_lookup_layer('[START]').numpy()\n",
    "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])\n",
    "\n",
    "dec_result, dec_state = decoder(inputs = DecoderInput(new_tokens=first_token, \n",
    "                                                      enc_output=example_enc_output,\n",
    "                                                      mask=(example_tokens != 0)),\n",
    "                                state = example_enc_state)\n",
    "\n",
    "print(f'logits shape: (batch_size, t, output_vocab_size) {dec_result.logits.shape}')\n",
    "print(f'state shape: (batch_size, dec_units) {dec_state.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        self.name = 'masked_loss'\n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                from_logits=True, reduction='none')\n",
    "        \n",
    "    def __call__(self, y_true, y_pred):\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        mask = tf.cast(y_true != 0, tf.float32)\n",
    "        loss *= mask\n",
    "        return tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTranslator(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units,\n",
    "                input_text_processor,\n",
    "                output_text_processor,\n",
    "                use_tf_function=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "                         embedding_dim, \n",
    "                         units)\n",
    "        decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                         embedding_dim,\n",
    "                         units)\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "        self.use_tf_function = use_tf_function\n",
    "        \n",
    "    def train_step(self, inputs):\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_train_step(inputs)\n",
    "        else:\n",
    "            return self._train_steps(inputs)\n",
    "        \n",
    "    def _preprocess(self, input_text, target_text):\n",
    "        input_tokens = self.input_text_processor(input_text)\n",
    "        target_tokens = self.output_text_processor(target_text)\n",
    "        \n",
    "        input_mask = input_tokens != 0\n",
    "        target_mask = target_tokens != 0\n",
    "        \n",
    "        return input_tokens, input_mask, target_tokens, target_mask\n",
    "    \n",
    "    def _train_steps(self, inputs):\n",
    "        input_text, target_text = inputs\n",
    "        \n",
    "        (input_tokens, input_mask,\n",
    "         target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
    "        \n",
    "        max_target_length = tf.shape(target_tokens)[1]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_state = self.encoder(input_tokens)\n",
    "            dec_state = enc_state\n",
    "            loss = tf.constant(0.0)\n",
    "            \n",
    "        for t in tf.range(max_target_length-1):\n",
    "            new_tokens = target_tokens[:, t:t+2]\n",
    "            step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
    "                                             enc_output, dec_state)\n",
    "            loss = loss + step_loss\n",
    "            \n",
    "        average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
    "            \n",
    "        variables = self.trainable_variables \n",
    "        gradients = tape.gradient(average_loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        return {'batch_loss': average_loss}\n",
    "    \n",
    "    def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
    "        input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
    "\n",
    "        # Run the decoder one step.\n",
    "        decoder_input = DecoderInput(new_tokens=input_token,\n",
    "                                   enc_output=enc_output,\n",
    "                                   mask=input_mask)\n",
    "\n",
    "        dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
    "\n",
    "        # `self.loss` returns the total for non-padded tokens\n",
    "        y = target_token\n",
    "        y_pred = dec_result.logits\n",
    "        step_loss = self.loss(y, y_pred)\n",
    "\n",
    "        return step_loss, dec_state\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = TrainTranslator(\n",
    "    embedding_dim, units,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor,\n",
    "    use_tf_function=False)\n",
    "\n",
    "# Configure the loss and optimizer\n",
    "translator.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=MaskedLoss(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['encoder_1/embedding_2/embeddings:0', 'encoder_1/gru_2/gru_cell_2/kernel:0', 'encoder_1/gru_2/gru_cell_2/recurrent_kernel:0', 'encoder_1/gru_2/gru_cell_2/bias:0', 'decoder_1/embedding_3/embeddings:0', 'decoder_1/gru_3/gru_cell_3/kernel:0', 'decoder_1/gru_3/gru_cell_3/recurrent_kernel:0', 'decoder_1/gru_3/gru_cell_3/bias:0', 'decoder_1/bahdanau_attention_2/dense_6/kernel:0', 'decoder_1/bahdanau_attention_2/dense_7/kernel:0', 'decoder_1/bahdanau_attention_2/additive_attention_2/scale:0', 'decoder_1/dense_8/kernel:0', 'decoder_1/dense_9/kernel:0', 'decoder_1/dense_9/bias:0'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-50b84dc1e72d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexample_input_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexample_target_batch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-159fcac6d1b3>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tf_train_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-159fcac6d1b3>\u001b[0m in \u001b[0;36m_train_steps\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mvariables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maverage_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'batch_loss'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0maverage_loss\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    628\u001b[0m       \u001b[0mRuntimeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mcalled\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcross\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mreplica\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m     \"\"\"\n\u001b[1;32m--> 630\u001b[1;33m     \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_empty_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\utils.py\u001b[0m in \u001b[0;36mfilter_empty_gradients\u001b[1;34m(grads_and_vars)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0m\u001b[0;32m     76\u001b[0m                      ([v.name for _, v in grads_and_vars],))\n\u001b[0;32m     77\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: ['encoder_1/embedding_2/embeddings:0', 'encoder_1/gru_2/gru_cell_2/kernel:0', 'encoder_1/gru_2/gru_cell_2/recurrent_kernel:0', 'encoder_1/gru_2/gru_cell_2/bias:0', 'decoder_1/embedding_3/embeddings:0', 'decoder_1/gru_3/gru_cell_3/kernel:0', 'decoder_1/gru_3/gru_cell_3/recurrent_kernel:0', 'decoder_1/gru_3/gru_cell_3/bias:0', 'decoder_1/bahdanau_attention_2/dense_6/kernel:0', 'decoder_1/bahdanau_attention_2/dense_7/kernel:0', 'decoder_1/bahdanau_attention_2/additive_attention_2/scale:0', 'decoder_1/dense_8/kernel:0', 'decoder_1/dense_9/kernel:0', 'decoder_1/dense_9/bias:0']."
     ]
    }
   ],
   "source": [
    "for n in range(10):\n",
    "    print(translator.train_step([example_input_batch, example_target_batch]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
